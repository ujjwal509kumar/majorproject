{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5089b-4844-4190-bab2-a43ba00bd887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\n",
    "\n",
    "\n",
    "# Set the path to your original dataset (update this path as needed)\n",
    "base_dir = r'C:\\Users\\91947\\Desktop\\dap and iot\\test\\pro'     \n",
    "output_base_dir = r'C:\\Users\\91947\\Desktop\\dap and iot\\test\\result\\pro'\n",
    "\n",
    "# List of categories (subfolder names)\n",
    "categories = [\"Normal\", \"Osteoporosis\", \"Osteopenia\"]\n",
    "\n",
    "# Number of augmented images to generate per original image\n",
    "n_aug_per_image = 5\n",
    "\n",
    "# Create an instance of ImageDataGenerator with desired augmentation parameters.\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Prepare output directories and report dictionary\n",
    "report = {}\n",
    "\n",
    "for category in categories:\n",
    "    # Define input and output paths for the current category\n",
    "    input_path = os.path.join(base_dir, category)\n",
    "    output_path = os.path.join(output_base_dir, category)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Get list of image files (assuming jpg or png files)\n",
    "    image_files = [f for f in os.listdir(input_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    original_count = len(image_files)\n",
    "    \n",
    "    # Counter for naming augmented files sequentially (starting at 1)\n",
    "    counter = 1\n",
    "    \n",
    "    # Process each image file in the folder\n",
    "    for file_name in image_files:\n",
    "        file_path = os.path.join(input_path, file_name)\n",
    "        \n",
    "        try:\n",
    "            # Load image and convert to numpy array\n",
    "            img = load_img(file_path)\n",
    "            x = img_to_array(img)\n",
    "            # Reshape to (1, height, width, channels)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            \n",
    "            # Generate augmented images for each original image\n",
    "            aug_iter = datagen.flow(x, batch_size=1)\n",
    "            for i in range(n_aug_per_image):\n",
    "                # Generate one augmented image\n",
    "                batch = next(aug_iter)\n",
    "                aug_img = array_to_img(batch[0])\n",
    "                # Create file name with the desired format (e.g., Normal_1.jpg, Normal_2.jpg, etc.)\n",
    "                new_file_name = f\"{category}_{counter}.jpg\"\n",
    "                save_path = os.path.join(output_path, new_file_name)\n",
    "                aug_img.save(save_path)\n",
    "                counter += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Calculate the number of augmented images generated for this category\n",
    "    augmented_count = counter - 1  # since counter started at 1\n",
    "    report[category] = {\"Original Images\": original_count, \"Augmented Images\": augmented_count}\n",
    "\n",
    "# %% [code]\n",
    "# Print a summary report\n",
    "print(\"Data Augmentation Report:\")\n",
    "for category, counts in report.items():\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    print(f\" - Original Images: {counts['Original Images']}\")\n",
    "    print(f\" - Augmented Images Generated: {counts['Augmented Images']}\")\n",
    "    increase = counts['Augmented Images'] - counts['Original Images']\n",
    "    print(f\" - Total Additional Images: {increase}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ec3f05-75eb-4a52-b715-b0f0d4eab108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning...\n",
      "\n",
      "Processing folder: C:\\Users\\91947\\Desktop\\dap and iot\\test\\result\\pro\\Normal\n",
      "\n",
      "Processing folder: C:\\Users\\91947\\Desktop\\dap and iot\\test\\result\\pro\\Osteoporosis\n",
      "\n",
      "Processing folder: C:\\Users\\91947\\Desktop\\dap and iot\\test\\result\\pro\\Osteopenia\n",
      "\n",
      "Data Cleaning Completed.\n",
      "Total files processed: 8211\n",
      "Total files removed: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import hashlib\n",
    "\n",
    "# Path to your dataset directory\n",
    "base_dir = r'C:\\Users\\91947\\Desktop\\dap and iot\\test\\result\\pro' \n",
    "\n",
    "# List of class folders\n",
    "categories = ['Normal', 'Osteoporosis', 'Osteopenia']\n",
    "\n",
    "# Dictionaries to store image hashes for each class to identify duplicates\n",
    "unique_hashes = {cat: {} for cat in categories}\n",
    "\n",
    "# Counters for reporting\n",
    "total_files = 0\n",
    "removed_files = 0\n",
    "\n",
    "print(\"Starting data cleaning...\")\n",
    "\n",
    "for cat in categories:\n",
    "    cat_dir = os.path.join(base_dir, cat)\n",
    "    print(f\"\\nProcessing folder: {cat_dir}\")\n",
    "    # List image files (adjust extensions as needed)\n",
    "    image_files = [f for f in os.listdir(cat_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        total_files += 1\n",
    "        img_path = os.path.join(cat_dir, img_file)\n",
    "        \n",
    "        # Try to read the image\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Corrupt or unreadable image: {img_path}. Removing file.\")\n",
    "            os.remove(img_path)\n",
    "            removed_files += 1\n",
    "            continue\n",
    "        \n",
    "        # Compute hash of the image (using PNG encoding for consistency)\n",
    "        try:\n",
    "            success, buffer = cv2.imencode('.png', img)\n",
    "            if not success:\n",
    "                print(f\"Encoding failed for image: {img_path}. Removing file.\")\n",
    "                os.remove(img_path)\n",
    "                removed_files += 1\n",
    "                continue\n",
    "            img_hash = hashlib.md5(buffer.tobytes()).hexdigest()\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing hash for {img_path}: {e}. Removing file.\")\n",
    "            os.remove(img_path)\n",
    "            removed_files += 1\n",
    "            continue\n",
    "        \n",
    "        # Check for duplicates: if the same hash exists, remove the duplicate file.\n",
    "        if img_hash in unique_hashes[cat]:\n",
    "            print(f\"Duplicate image found: {img_path}. Removing duplicate.\")\n",
    "            os.remove(img_path)\n",
    "            removed_files += 1\n",
    "        else:\n",
    "            unique_hashes[cat][img_hash] = img_path\n",
    "\n",
    "print(\"\\nData Cleaning Completed.\")\n",
    "print(f\"Total files processed: {total_files}\")\n",
    "print(f\"Total files removed: {removed_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38508fa-fb97-4d67-b4f7-5a66ef5b899f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
